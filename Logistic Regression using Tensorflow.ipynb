{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import numpy.random as rnd\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TensorBoard stuff\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "n_epochs = 5000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Load and scale dataset\n",
    "moons = make_moons()\n",
    "moons_plus_bias = np.c_[np.ones((moons[0].shape[0], 1)), moons[0]]\n",
    "scaled_moons_plus_bias = StandardScaler().fit_transform(moons_plus_bias)\n",
    "\n",
    "# Define batch size and number of batches\n",
    "m, n = moons[0].shape\n",
    "batch_size = 100\n",
    "n_batches = int(np.ceil(m / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Mini-batch Gradient Descent\n",
    "def fetch_batch(epoch, batch_index, batch_size):\n",
    "    rnd.seed(epoch * n_batches + batch_index)\n",
    "    indices = rnd.randint(m, size=batch_size)\n",
    "    X_batch = scaled_moons_plus_bias[indices]\n",
    "    y_batch = moons[1].reshape(-1, 1)[indices]\n",
    "    return X_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression():\n",
    "    with tf.name_scope(\"inputs\") as scope:\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\")\n",
    "        y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\")\n",
    "    \n",
    "    theta = tf.Variable(tf.random_uniform([moons[0].shape[1] + 1, 1], -1.0, 1.0), name=\"theta\")\n",
    "    y_pred = tf.nn.sigmoid(tf.matmul(X, theta, name=\"predictions\"))\n",
    "    \n",
    "    with tf.name_scope(\"loss\") as scope:\n",
    "        error = (-y * tf.log(y_pred) - (1-y) * tf.log(1 - y_pred)) \n",
    "        cost = tf.reduce_mean(error, name = \"cost\")\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Summary for plotting\n",
    "    cost_summary = tf.summary.scalar('MSE', cost)\n",
    "    file_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())\n",
    "    \n",
    "    # Train linear regression model using mini-batch gradient descent\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_index in range(n_batches):\n",
    "                X_batch, y_batch = fetch_batch(epoch, batch_index, batch_size)\n",
    "                if batch_index % 10 == 0:\n",
    "                    summary_str = cost_summary.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "                    step = epoch * n_batches + batch_index\n",
    "                    file_writer.add_summary(summary_str, step)\n",
    "                sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        best_theta = theta.eval()\n",
    "        # Save the best model\n",
    "        save_path = saver.save(sess, \"logistic_regression.ckpt\")\n",
    "        file_writer.close()\n",
    "    \n",
    "    return best_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.75642967e-03],\n",
       "       [  1.00682271e+00],\n",
       "       [ -2.51997995e+00]], dtype=float32)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
